---
title: Lambda & Step Functions
weight: 240
pre: "<b>2.4. </b>"
---

# 2.4. Setup an AWS Glue ETL pipeline

![Data Lake Architecture](/images/modules/lambda.png?width=50pc)

> {{%expand "🎯🎯🎯 Deploy into Production ELT Pipeline" %}}
In the previous sections, you explored Raw NYC Taxi Trips Dataset. You then developed and tested ETL code to transform the Raw NYC Taxi Trips Dataset into a new Dataset that is optimized for querying and reporting by Unicorn-Taxi's Business End-Users. Now, it's time to deploy your Code into a Production ETL Pipeline using Glue.
{{% /expand%}}

### 2.4.1. Schedule AWS Glue Crawlers

> {{%expand "🎯We'll start by scheduling the Raw Dataset AWS Glue Crawler" %}}
In this section, we'll build a **Schedule-Driven** AWS Glue ETL pipeline that looks as follows. We work our way backwards from a daily data availability Service Level Agreement (SLA) goal.
Unicorn-Taxi's Business End-Users need their Datasets to be refreshed daily and available by that time.
{{% /expand%}}

1. Navigate to the [AWS Glue console. In the left menu, click **Crawlers**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#catalog:tab=crawlers)
2. Click on `nyctaxi-raw-crawler` , then click **Action** >> **Edit crawler**
3. In the left list of steps, click on **Schedule**
4. For **Frequency**, select `Daily`
5. Select `07:00 UTC` (2:00PM GMT+7)
6. In the left list, click on **Review all steps**
7. Scroll down and click **Finish**

> 🎯Next, we'll schedule the **Optimized** Dataset AWS Glue Crawler.

1. Navigate to the [AWS Glue console. In the left menu, click **Crawlers**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#catalog:tab=crawlers)
2. Click on `nyctaxi-optimized-crawler`, then click **Action** >> **Edit crawler**
3. In the left list of steps, click on **Schedule**
4. For **Frequency** , select `Daily`
5. Select `08:00 UTC` (3:00PM GMT+7)
6. In the left list, click on **Review all steps**
7. Scroll down and click **Finish**


### 2.4.2. Create an AWS Glue Job

The next step in setting up our AWS Glue ETL pipeline is to create an AWS Glue Job.

1. Navigate to the [AWS Glue console. In the left menu, under **ETL** , click **Jobs**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#etl:tab=jobs) >> Click **Add job**

2. In step **Job properties** ...

    a. For **Name**, enter `nyctaxi-create-optimized-dataset`
    
    b. For **IAM role**, select `AWSGlueServiceRole-nyctaxi-optimize-job`
    
    c. For **This job runs**, select `An existing script that you provide`
    
    d. For **S3 path where the script is stored**, copy-and-paste this S3 URL:
    ```
    ~~s3://serverless-data-lake-XXX~~/scripts/nyctaxi_create_optimized_dataset_job.py
    ```

    Replace ~~`serverless-data-lake-XXX`~~ with the actual name of your Amazon S3 bucket.

    e. For **Temporary directory**, specify the following S3 URL:
    ```
    ~~s3://serverless-data-lake-XXX~~/data/tmp
    ```

    f. Expand section **Advanced properties**
        
    👉 For **Job bookmark**, select `Enable`

    👉 For **Monitoring options** >> **Job metrics**, select `Enable`

    g. Expand section **Security configuration, script libraries, and job parameters (optional)**

    👉 For **Max concurrent** DPUs per Job run , enter 4
        
    👉 Find the section named **Job parameters**
    
    Under **Key**, enter `--your_bucket_name` (two dashes, then the word **your_bucket_name** )

    Under **Value**, enter your actual Amazon S3 bucket name ~~`serverless-data-lake-XXX`~~

    h. Click **Next**

3. ~~In step **Connections** , click **Next**~~
4. ~~In step **Review** , click **Save job and edit script**~~
5. In step **Connections** , click **Save job and edit script**
6. Review the script. Notice AWS Glue **job setup and teardown code**.
7. Click **X** on the top right to return to AWS Glue console. You have successfully created an AWS Glue job.

You can run this AWS Glue Job any time. With 4 DPUs allocated, it takes **about 7 - 9 minutes** from a cold start to complete. The job creates and writes an optimized NYC Taxi (Yellow) dataset to the following Amazon S3 path in your own account:

~~s3://serverless-data-lake-XXX~~/data/**prod**/nyctaxi/**yellow_rpt**

You can download the [Job's script here](https://github.com/nnthanh101/serverless-data-lake/blob/nyc-taxi/README/nyc-taxi/scripts/nyctaxi_create_optimized_dataset_job.py)

> {{%expand "✍️ TRY IT OUT LATER" %}}
You can have AWS Glue auto-generate an ETL script for you! You can then edit and customize the script to your needs.

To accomplish that, in **Step 2.c** of the previous exercise, for **This job runs,** try to select **A proposed script generated by AWS Glue** instead. Follow the steps to create a fully-functional ETL script.
{{% /expand%}}


### 2.4.3. Create an AWS Glue Trigger

Finally, we'll create a **Time-based** AWS Glue Trigger to trigger our job through the following steps.

1. Navigate to the AWS Glue console
2. In the left menu, under **ETL**, click **Triggers**, then click **Add trigger** button
3. In step **Trigger properties** ’...

    a. For **Name**, enter `nyctaxi-process-raw-dataset`

    b. For **Trigger type**, select `Schedule`
    
    c. For **Frequency**, select `Custom`
    
    d. For **Cron expression**, enter `15 07 ? * * *` (mind the spaces!)
    
    e. Click **Next**

4. In step **Jobs to start** ...

    a. Under **Job**, look for `nyctaxi-optimize-raw-dataset`. Click **Add** next to it.
    
    b. Leave **Job bookmark** set to `Enabled`
    
    c. Click **Next**

5. In step **Review all steps** , check **Enable trigger on creation**
6. Click **Finish**

{{% notice note %}} 
**Congratulations! You have successfully set up your AWS Glue ETL pipeline.**
The pipeline will run on schedule. You can check your AWS Glue console at the times you scheduled to ensure that your Crawlers and your AWS Glue Job have run.
{{% /notice %}}


### 2.4.4. Advanced Exercise: AWS Glue Job Bookmarks

🎯 Let's try out the AWS Glue's Job Bookmarks feature which enables production ETL jobs to process only new data when rerunning on a scheduled interval.

> 2.4.4..1. We'll start by simulating an initial run of the production ETL pipeline to process data files pertaining to **January & February 2017**.

1. Let the ETL Pipeline run once on the scheduled time.

    {{%expand "Alternatively, We can manually start the AWS Glue job" %}}
    Alternatively, you can manually start the AWS Glue job you've just created and wait for it to complete. 
    
    - Navigate to **Jobs** console. 
    - Check `nyctaxi-create-optimized-dataset`.
    - Click **Action** → **Run job**. 
    - Expand **Advanced properties**, and for **Job bookmark**, ensure `Enable` is selected. 
    - Click **Run**.

    When the Job completes, it will have created a new Production Dataset covering _January and February 2017_.
    {{% /expand%}}


2. Let's Crawl the new Production Dataset. 
   - Navigate to [**Crawlers console**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#catalog:tab=crawlers)
   - Re-run the Optimized Dataset Crawler: `nyctaxi-optimized-crawler`. 
   Crawler will detect the new Dataset and add it to a table named `yellow_rpt`.
3. Navigate to the [**Amazon Athena console**](https://ap-southeast-1.console.aws.amazon.com/athena/home?force&force=&region=ap-southeast-1#query). 
   - Ensure database `nyctaxi` is selected. 
   - **Refresh** tables list. 
   - **Copy, paste, and run** the following query against the newly-created production reporting dataset table, `yellow_rpt`.

```
SELECT count(*) AS march_count FROM yellow_rpt WHERE cast(pu_month AS BigInt) = 3 GROUP BY yellow_rpt.pu_month
```

Notice that there is a **Zero records returned.** for March.

> 2.4.4.2. Now, let's simulate as if a new file for **March 2017** was added to our raw dataset.

4. Navigate to the [**AWS Lambda** console. On the left menu, click **Functions**](https://ap-southeast-1.console.aws.amazon.com/lambda/home?region=ap-southeast-1#/functions)
5. In the **Functions** list, select function starting with **`copy_raw_nyctaxi_data`**
6. Click on **Test**
7. On the **Configure test event** dialog...

    a. Select `Create new test event`

    b. For **Event name**, enter `IngestMarch2017`

    c. For Event body , copy and replace existing body with the JSON text below:

    ```
    {
    "s3_prefix": "data/raw/nyctaxi/yellow/yellow_tripdata_2017-03.csv.bz2"
    }
    ```
    
    d. Click Create

8. Next, on the top right, click the **Test** button. Wait until you receive **Execution result: succeeded**. The March 2017 file has been ingested into your Amazon S3 bucket!

> 2.4.4.3. Finally, we'll manually re-run the Production ETL Job and verify that it processed the newly-ingested file for **March 2017**.

9. Navigate to the **AWS Glue** console. Click on **Jobs**
10. Manually run the **nyctaxi-create-optimized-dataset** job again. Wait for it to complete.
11. Navigate to **Crawlers**, and re-run the optimized dataset crawler **nyctaxi-optimized-crawler**.
12. Repeat **step #3**. Does it show a count for March 2017 records?

Optionally, you can also navigate to Amazon S3 console and browse the following locations and verify that the Glue Job did not re-process January and February data and append duplicate files. Check the number of files in the following S3 locations and ensure there are 31 objects for January and 28 objects for February:


* [x] s3://~~serverless-data-lake-XXX~~ /data/**prod**/nyctaxi/**yellow_rpt**/pu_year=2017/pu_month=1/

* [x] s3://~~serverless-data-lake-XXX~~ /data/**prod**/nyctaxi/**yellow_rpt**/pu_year=2017/pu_month=2/